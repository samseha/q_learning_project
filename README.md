# Q-Learning Project
Team members: Seha Choi and Shengjie Lin
## Implementation Plan
### Q-learning algorithm
* Executing the Q-learning algorithm

    We will just use the function we learned in class. For rewards we will put a positive reward for correct block placement at the AR code and a negative reward for the incorrect block placement. While the robot is in transition there will be 0 reward. We will test this by running a couple iterations and checking if they are the correct values.
* Determining when the Q-matrix has converged

     We will say the Q-matrix is converged if the values for each action are within 0.02 difference (the value is subject to change after testing). We will test this via printing out the Q-Matrix values and see if they are consistent.
* Once the Q-matrix has converged, how to determine which actions the robot should take to maximize expected reward

    We will take a greedy approach and pick the highest reward action. After the matrix has converged the actions should be pretty clear. I.e) Moving the block to the corresponding AR Code. We will verify that this is identical to taking a greedy approach.
### Robot perception
* Determining the identities and locations of the three colored objects

    We will process the image frames by computing a mask of the pixels corresponding to the specified color. Then we will find its center by computing the image moment. That will be the location of the colored object. To veriy this, we will visualize the center of the color patch.
* Determining the identities and locations of the three AR tags

    To detect AR tags, we will process the image frames uisng the `cv2.aruco` module. It will give us the ids and corner locations of the AR tags. To verify this, we will visualize the borders of the AR tags.
### Robot manipulation & movement
* Picking up and putting down the colored objects with the OpenMANIPULATOR arm

    Once the robot is ready at the appropriate location, we will use `MoveIt` package to manipulate the robot arm to pick up the object. To verify this, we will check whether the manipulation is successful in the simulator.
* Navigating to the appropriate locations to pick up and put down the colored objects

     We will let the turtlebot self-rotate until it sees pixels of the specified color. Then we will let it move forward to a certain distance from the object while steering itself by trying to center the colored patch, both of which are achieved by using a p-controller. To verify this, we will check whether the robot exhibits the desired behavior in simulation.
### TimeLine
Q-Learning Algorithm - By May 1st. We will finish the Q_Learning algorithm part by May 1st and see if the matrix converges to the expected values.

Robot Perception - By May 4th. Since the Q-Learning Algorithm is separate from the Robot movements we should be able to test independently that the robot is able to execute actions via perception.

Robot Manipulation & Movement - By May 4th. Similar to above since this is separate from the Q-Learning algorithm we could work in parallel and divide the code up.

Final wrap up - By May 8th. We will see if the matrix converges correctly and if the robot is able to execute the commands generated by the matrix correctly.
## Write up
### Objectives
We train a Q-matrix based on the rewards of object placement to the tags and let it converge. With the Q-Matrix we find the optimal actions and execute it on the physical Turtlebot using arm and motor movement.
### High-level description
We first use the Q-Learning Algorithm to determine which object belongs to which tag. We will save that matrix and in our actions file we will load it. We will use a greedy algorithm to select the most optimal actions and use the sensors of the robot to detect colored objects and the corresponding AR code to place the objec to the desired AR code. For directional control we will use a proportional controller and to pick up and drop objects we will let the arm move into pre-conceived angles.
### Q-Learning Algorithm
We first initialize an empty Q matrix and set all values to 0. We then populate invalid state action pairs to a value of -1. In addition, we create a transition matrix which tells us which state an action will lead to. i.e) transtiion matrix[state, action] = new state
#### Update Q Matrix
We just follow the Q-learning algorithm and update the corresponding values using the reward that is received after performing a valid action. For selecting action we just uniformly select an action among the valid actions.
#### Testing Convergenge
We test to see if the values in the Q-Matrix does not change after a certain amount of movements. A Q value is regarded as unchanged if the difference is less than a threshold of 0.5.
### Robot Actions
We use a greedy approach to select the optimal actions from each state. Then with the selected actions we execute it on the robot via camera recognition, LIDAR recognition, AR code recognition, and robot arm movements.
#### Camera Recognition
We first use the camera and set a mask for each of the colors. We take the X component from the recognized color and use a proportion controller so that the robot will face towards the bar move towards it. If the robot does not detect any desired object within its reading frame it will just spin in its place until it finds a desired object.
#### Lidar Recognition
We take into consideration of the closests object of the robot that is within a -15,15 degree angle. The reasoning behind it is that since the robot will be facing towards the desired object due to the camera recognition, other objects that are close to the robot, not infront of it, must be ignored. We have 2 different target distances for picking up objects and dropping off objects, with the latter being a bit larger. Once the robot reaches our desired target distance, it will move on to the arm movement phase.
#### Robot Arm Movement
We measured out specific joint and gripper values using R-Viz. Once the robot is ready to pick up it will execute it by opening the gripper, moving down, close the gripper, then raising the arm so it doesn't block camera and Lidar sensors. We do the opposite steps for laying down an object. In addition, after picking up or dropping down an object we do a movement called "reset to middle". The robot will back out and do a 180 degree turn so that it faces the other side. Although the robot will work without this movement, we did it for convenience.
## Challenges
The biggest challenge for us was finding the k value to use for the proportional controller and the lag we experienced with robots. For some reason, some of the turtlebots had major lag and it took multiple seconds for our velocity to push making our robot just spin in circles. At first we thought it was an issue with our code but after changing between three robots (we checked that it worked on Gazebo as it should), the third robot worked perfectly.
## Future Works
Right now when we do not detect a desired object, we just spin in one direction and this could take a lot of time since the robot might have to turn 360 degrees. In the future we could maybe remember where certain objects are and make the robot spin in a direction that is closest to the next desired object.
## Takeaways
Our biggest take away is that observations can be noisy and we have to make it robust to these noise. For example, we put a time stamp for visual detection so that we would only take in detections that was within a certain timeframe. In addition, we also checked if the pixel detected for the colored objects was significant enough. Since sometimes lighting could affect the detection of the colored objects. We tested on different lighting condition and put safety measures like these to make it more robust to noise.

https://user-images.githubusercontent.com/57845592/167313776-672f892e-8c52-417a-bb0a-1c7f6ebb56e7.mp4

